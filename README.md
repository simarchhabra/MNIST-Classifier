Gradient-based Neural Network written from scratch without use of any external
Machine Learning libraries that recognizes handwritten digits from MNIST data.
Optimized using L2 Regularization, Stochastic gradient descent, Cross-Entropy 
cost, early stopping and parameters optimized using bayesian hyperparameter 
optimization, specifically Hyperopt's tree-strucuted parzen estimator. Produces
up to 99.99% training accuracy and up to 98.1% testing accuracy for a 3 layer 
network. More hidden layer networks is possible using this program, but only 
optimized to 97.5% testing accuracy. Hyperparameter optimization not done for 
greater than 3 layer networks.
